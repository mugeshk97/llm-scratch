{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, max_length= 1, stride = 1):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "\n",
    "        token_ids = tokenizer.encode(data, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to create overlapping sequences\n",
    "        for i in range(0, len(token_ids) - 1, stride):  # Adjusted to prevent IndexError\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: min(len(token_ids), i + max_length + 1)]\n",
    "\n",
    "            # Ensure chunks have the same length\n",
    "            if len(input_chunk) == max_length and len(target_chunk) == max_length:\n",
    "                self.input_ids.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "                self.target_ids.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# load the data train , validation, test\n",
    "train_data = raw_text[:int(len(raw_text)*0.8)]\n",
    "valid_data = raw_text[int(len(raw_text)*0.8):int(len(raw_text)*0.9)]\n",
    "test_data = raw_text[int(len(raw_text)*0.9):]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_data, max_length=4, stride=4)\n",
    "valid_dataset = TextDataset(valid_data, max_length=4, stride=4)\n",
    "test_dataset = TextDataset(test_data, max_length=4, stride=4)\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"context_length\": 1024,\n",
    "    \"qkv_bias\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, drop_rate=0.1, qkv_bias=False):\n",
    "        super(CausalMultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim  # Embedding dimension\n",
    "        self.n_heads = n_heads  # Number of attention heads\n",
    "        self.head_dim = emb_dim // n_heads  # Dimension per head\n",
    "        self.scale = 1 / (self.head_dim ** 0.5)  # Scaling factor for attention scores\n",
    "        \n",
    "        # Linear layer to compute Query, Key, and Value (concatenated as a single tensor)\n",
    "        self.qkv = nn.Linear(emb_dim, emb_dim * 3, bias=qkv_bias)\n",
    "        \n",
    "        # Dropout for attention weights\n",
    "        self.attn_drop = nn.Dropout(drop_rate)\n",
    "        \n",
    "        # Linear layer for output projection\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
    "        \n",
    "        # Dropout for the final output\n",
    "        self.proj_drop = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape  # B = batch size, N = sequence length, C = embedding dimension\n",
    "        \n",
    "        # Compute Q, K, V by projecting input and reshaping\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Extract Query, Key, and Value tensors\n",
    "        \n",
    "        # Compute attention scores: Q @ K^T (scaled dot product)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # Shape: (B, n_heads, N, N)\n",
    "        \n",
    "        # Causal mask to prevent attending to future tokens\n",
    "        mask = torch.triu(torch.ones(N, N, device=x.device), diagonal=1).bool()\n",
    "        attn.masked_fill_(mask, float('-inf'))  # Set future positions to -inf before softmax\n",
    "        \n",
    "        # Apply softmax to normalize attention scores\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        # Apply dropout to attention weights\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        # Compute weighted sum of Value vectors\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        \n",
    "        # Final linear projection of attention output\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        # Apply dropout to the final output\n",
    "        out = self.proj_drop(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim, expansion=4, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, emb_dim * expansion)  # Expand dimension\n",
    "        self.fc2 = nn.Linear(emb_dim * expansion, emb_dim)  # Project back\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))  # Apply GELU activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Project back to original dimension\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, drop_rate, qkv_bias):\n",
    "        super().__init__()\n",
    "        self.attn = CausalMultiHeadAttention(emb_dim, n_heads, drop_rate, qkv_bias)\n",
    "        self.ff = FeedForward(emb_dim, expansion=4, drop_rate=drop_rate)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.drop_shortcut = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention layer with residual connection\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Residual connection\n",
    "\n",
    "        # Feed-forward layer with residual connection\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Residual connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaLLM(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg[\"emb_dim\"], cfg[\"n_heads\"], cfg[\"drop_rate\"], cfg[\"qkv_bias\"]) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = BetaLLM(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "# Initialize the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (input_ids, target_ids) in enumerate(train_dataloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Compute loss using CrossEntropyLoss\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"beta_llm.pth\")\n",
    "\n",
    "\n",
    "\n",
    "# add validation and test loop\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(dataloader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Compute loss using CrossEntropyLoss\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "# Validation loop\n",
    "valid_loss = evaluate(model, valid_dataloader, criterion)\n",
    "print(f\"Validation Loss: {valid_loss}\")\n",
    "# Test loop\n",
    "test_loss = evaluate(model, test_dataloader, criterion)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"beta_llm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a land far away, and in the first I had always sh that I had always in the first time my own sitters had always sh that I had always been the man, and in the first I had always the portrait.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = BetaLLM(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"beta_llm.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# tokenizer\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "def generate_text(model, tokenizer, prompt, max_length=50):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    input_ids = torch.tensor(tokens).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[:, -1, :]  # Get logits for the last token\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)  # Sample the next token\n",
    "            input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)  # Append to input\n",
    "\n",
    "    generated_tokens = input_ids.squeeze().tolist()\n",
    "    generated_text = tokenizer.decode(generated_tokens)\n",
    "    return generated_text\n",
    "# Example usage\n",
    "prompt = \"Once upon a time in a land far away\"\n",
    "\n",
    "generated_text = generate_text(model, tokenizer, prompt, max_length=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
