{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, max_length= 1, stride = 1):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "\n",
    "        token_ids = tokenizer.encode(data, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to create overlapping sequences\n",
    "        for i in range(0, len(token_ids) - 1, stride):  # Adjusted to prevent IndexError\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: min(len(token_ids), i + max_length + 1)]\n",
    "\n",
    "            # Ensure chunks have the same length\n",
    "            if len(input_chunk) == max_length and len(target_chunk) == max_length:\n",
    "                self.input_ids.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "                self.target_ids.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# load the data train , validation, test\n",
    "train_data = raw_text[:int(len(raw_text)*0.8)]\n",
    "valid_data = raw_text[int(len(raw_text)*0.8):int(len(raw_text)*0.9)]\n",
    "test_data = raw_text[int(len(raw_text)*0.9):]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_data, max_length=4, stride=4)\n",
    "valid_dataset = TextDataset(valid_data, max_length=4, stride=4)\n",
    "test_dataset = TextDataset(test_data, max_length=4, stride=4)\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_layers\": 2,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"context_length\": 4,\n",
    "    \"qkv_bias\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, drop_rate=0.1, qkv_bias=False):\n",
    "        super(CausalMultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim  # Embedding dimension\n",
    "        self.n_heads = n_heads  # Number of attention heads\n",
    "        self.head_dim = emb_dim // n_heads  # Dimension per head\n",
    "        self.scale = 1 / (self.head_dim ** 0.5)  # Scaling factor for attention scores\n",
    "        \n",
    "        # Linear layer to compute Query, Key, and Value (concatenated as a single tensor)\n",
    "        self.qkv = nn.Linear(emb_dim, emb_dim * 3, bias=qkv_bias)\n",
    "        \n",
    "        # Dropout for attention weights\n",
    "        self.attn_drop = nn.Dropout(drop_rate)\n",
    "        \n",
    "        # Linear layer for output projection\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
    "        \n",
    "        # Dropout for the final output\n",
    "        self.proj_drop = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape  # B = batch size, N = sequence length, C = embedding dimension\n",
    "        \n",
    "        # Compute Q, K, V by projecting input and reshaping\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Extract Query, Key, and Value tensors\n",
    "        \n",
    "        # Compute attention scores: Q @ K^T (scaled dot product)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # Shape: (B, n_heads, N, N)\n",
    "        \n",
    "        # Causal mask to prevent attending to future tokens\n",
    "        mask = torch.triu(torch.ones(N, N, device=x.device), diagonal=1).bool()\n",
    "        attn.masked_fill_(mask, float('-inf'))  # Set future positions to -inf before softmax\n",
    "        \n",
    "        # Apply softmax to normalize attention scores\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        # Apply dropout to attention weights\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        # Compute weighted sum of Value vectors\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        \n",
    "        # Final linear projection of attention output\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        # Apply dropout to the final output\n",
    "        out = self.proj_drop(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim, expansion=4, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, emb_dim * expansion)  # Expand dimension\n",
    "        self.fc2 = nn.Linear(emb_dim * expansion, emb_dim)  # Project back\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))  # Apply GELU activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Project back to original dimension\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, drop_rate, qkv_bias):\n",
    "        super().__init__()\n",
    "        self.attn = CausalMultiHeadAttention(emb_dim, n_heads, drop_rate, qkv_bias)\n",
    "        self.ff = FeedForward(emb_dim, expansion=4, drop_rate=drop_rate)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.drop_shortcut = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention layer with residual connection\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Residual connection\n",
    "\n",
    "        # Feed-forward layer with residual connection\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Residual connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaLLM(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg[\"emb_dim\"], cfg[\"n_heads\"], cfg[\"drop_rate\"], cfg[\"qkv_bias\"]) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 11.022274017333984\n",
      "Epoch 0, Batch 10, Loss: 10.517340660095215\n",
      "Epoch 0, Batch 20, Loss: 10.002131462097168\n",
      "Epoch 0, Batch 30, Loss: 9.26123332977295\n",
      "Epoch 0, Batch 40, Loss: 8.107023239135742\n",
      "Epoch 0, Batch 50, Loss: 9.200823783874512\n",
      "Epoch 0, Batch 60, Loss: 8.3155517578125\n",
      "Epoch 0, Batch 70, Loss: 7.320669651031494\n",
      "Epoch 0, Batch 80, Loss: 8.815411567687988\n",
      "Epoch 0, Batch 90, Loss: 7.31934928894043\n",
      "Epoch 0, Batch 100, Loss: 7.8155903816223145\n",
      "Epoch 0, Batch 110, Loss: 7.6741437911987305\n",
      "Epoch 0, Batch 120, Loss: 7.760836124420166\n",
      "Epoch 1, Batch 0, Loss: 5.722585201263428\n",
      "Epoch 1, Batch 10, Loss: 6.676328182220459\n",
      "Epoch 1, Batch 20, Loss: 6.01940393447876\n",
      "Epoch 1, Batch 30, Loss: 5.963046073913574\n",
      "Epoch 1, Batch 40, Loss: 5.989701747894287\n",
      "Epoch 1, Batch 50, Loss: 6.153065204620361\n",
      "Epoch 1, Batch 60, Loss: 5.658124923706055\n",
      "Epoch 1, Batch 70, Loss: 5.895581245422363\n",
      "Epoch 1, Batch 80, Loss: 6.4672369956970215\n",
      "Epoch 1, Batch 90, Loss: 5.389408111572266\n",
      "Epoch 1, Batch 100, Loss: 6.067648887634277\n",
      "Epoch 1, Batch 110, Loss: 5.664520263671875\n",
      "Epoch 1, Batch 120, Loss: 5.108953952789307\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = BetaLLM(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "# Initialize the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (input_ids, target_ids) in enumerate(train_dataloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Compute loss using CrossEntropyLoss\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"beta_llm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.9204020500183105\n",
      "Test Loss: 6.3660045231089875\n"
     ]
    }
   ],
   "source": [
    "# add validation and test loop\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(dataloader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Compute loss using CrossEntropyLoss\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "# Validation loop\n",
    "valid_loss = evaluate(model, valid_dataloader, criterion)\n",
    "print(f\"Validation Loss: {valid_loss}\")\n",
    "# Test loop\n",
    "test_loss = evaluate(model, test_dataloader, criterion)\n",
    "print(f\"Test Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " left the like portrait\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_layers\": 2,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"context_length\": 4,  # Important: model can only process 4 tokens at a time\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "model = BetaLLM(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"beta_llm.pth\"))\n",
    "model.to(device)  # Move model to the correct device\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "\n",
    "# Use context_length from model config\n",
    "MAX_CONTEXT_LENGTH = GPT_CONFIG_124M[\"context_length\"]\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=4, top_k=5, temperature=1.0):\n",
    "    tokens = tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "    # Ensure input does not exceed model's context length\n",
    "    if len(tokens) > MAX_CONTEXT_LENGTH:\n",
    "        print(f\"Warning: Prompt is too long. Truncating to {MAX_CONTEXT_LENGTH} tokens.\")\n",
    "        tokens = tokens[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    input_ids = torch.tensor(tokens).unsqueeze(0).to(device)  # Ensure input tensor is on the correct device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[:, -1, :]  # Get logits for the last token\n",
    "\n",
    "            # Apply temperature scaling\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "\n",
    "            # Apply top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, k=top_k)\n",
    "            top_k_probs = torch.nn.functional.softmax(top_k_logits, dim=-1)\n",
    "\n",
    "            # Sample from the top-k distribution\n",
    "            sampled_index = torch.multinomial(top_k_probs, num_samples=1)\n",
    "\n",
    "            # Convert back to the original vocabulary indices\n",
    "            next_token = top_k_indices.gather(-1, sampled_index).squeeze(-1)\n",
    "\n",
    "            # Ensure input_ids has the correct shape\n",
    "            input_ids = torch.cat((input_ids[:, 1:], next_token.unsqueeze(1)), dim=1)  # Shift context\n",
    "\n",
    "            # Ensure special tokens are allowed for encoding\n",
    "            end_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "            # Check if the model has generated the end token\n",
    "            if next_token.item() == end_token:\n",
    "                break\n",
    "\n",
    "    generated_tokens = input_ids.squeeze().tolist()\n",
    "    generated_text = tokenizer.decode(generated_tokens)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_text(model, tokenizer, prompt, max_length=4, top_k=100, temperature=1000.5)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
